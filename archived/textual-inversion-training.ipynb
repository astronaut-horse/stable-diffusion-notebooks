{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual Inversion Training \n",
    "# -----------------------------------\n",
    "# This notebook was built for stable diffusion fine-tuning\n",
    "# It takes 3-8 training image urls as it's primary input\n",
    "# Additionally it can take any number of text prompts\n",
    "# It outputs any specified number of images\n",
    "# Output images are saved in Google Drive\n",
    "# -----------------------------------\n",
    "# You can run as a Google Colab Notebook\n",
    "# Not 100% cleaned up, definitely a work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources / Reference Notebooks\n",
    "# -----------------------------------\n",
    "# https://huggingface.co/docs/diffusers/training/text_inversion\n",
    "# https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb\n",
    "# https://www.youtube.com/watch?v=_7rMfsA24Ls\n",
    "# https://huggingface.co/\n",
    "# https://www.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Images URL's\n",
    "# -----------------------------------\n",
    "# Paste in urls for the images you want to train your new concept on.\n",
    "# These could be images that inspire you or images of your artwork etc.\n",
    "# Upload your own images to https://imgur.com/ for urls\n",
    "# 5-8 images is usually adequate for a training.\n",
    "# Follow the format below.\n",
    "\n",
    "# urls = [\n",
    "#   \"https://i.imgur.com/xNZW4TW.jpg\",\n",
    "#   \"https://i.imgur.com/9NMokla.jpg\",\n",
    "#   \"https://i.imgur.com/A4p0djw.jpg\",\n",
    "#   \"https://i.imgur.com/ocYQtxp.jpg\",\n",
    "#   \"https://i.imgur.com/CuchxlL.jpg\",\n",
    "#   \"https://i.imgur.com/9uDvP2j.jpg\",\n",
    "#   \"https://i.imgur.com/63W9Cyv.jpg\",\n",
    "# ]\n",
    "\n",
    "urls = [] # <----- paste your image urls here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Prompts\n",
    "# -----------------------------------\n",
    "# Add as many text prompts as desired.\n",
    "# All text prompts will be run after initial training.\n",
    "# Output images will go to your specified Google Drive folder.\n",
    "# All prompts will be appended with \"in the style of <your-model>\"\n",
    "# More prompts can be given later.\n",
    "\n",
    "prompts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Information / Variables\n",
    "# -----------------------------------\n",
    "# [needs refactoring]\n",
    "\n",
    "\n",
    "# Variables for model outputs...\n",
    "images_per_prompt = 5\n",
    "total_output_images = 175\n",
    "ahx_model_number = 11\n",
    "\n",
    "what_to_teach = \"style\" # [\"object\", \"style\"]\n",
    "# `initializer_token` is a word that can summarise what your new concept is, to be used as a starting point\n",
    "initializer_token = \"painting\"\n",
    "\n",
    "\n",
    "\n",
    "# ------ DON'T CHANGE THESE ----------------\n",
    "\n",
    "# Variables to save new model to hugging face...\n",
    "save_concept_to_public_library = True\n",
    "name_of_your_concept = f\"ahx-model-{ahx_model_number}\"\n",
    "name_of_your_concept_dup = f\"{name_of_your_concept}\" # temporary for sanity check before uploading to concept library\n",
    "hf_token_write = \"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\"\n",
    "\n",
    "# Mount log in to google drive to save images...\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') # <-- shouldn't change\n",
    "root_path = '/content/drive/My Drive' # <-- shouldn't change\n",
    "\n",
    "# Make your folder in Google Drive and define here...\n",
    "your_path = f'/stable-diffusion/model-{ahx_model_number}-bulk' # <-- your folder\n",
    "google_drive_path = f'{root_path}{your_path}'\n",
    "\n",
    "# `placeholder_token` is the token you are going to use to represent your new concept (so when you prompt the model, you will say \"A `<my-placeholder-token>` in an amusement park\"). We use angle brackets to differentiate a token from other words/tokens, to avoid collision.\n",
    "placeholder_token = f\"<{name_of_your_concept}>\" # {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Installations\n",
    "# -----------------------------------\n",
    "# [boiler plate / needs refactoring]\n",
    "\n",
    "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
    "!pip install -qq accelerate transformers ftfy\n",
    "!pip install -qq \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Optimization\n",
    "# -----------------------------------\n",
    "# [boiler plate / needs refactoring]\n",
    "\n",
    "\n",
    "!pip install -U --pre triton\n",
    "\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "s = getoutput('nvidia-smi')\n",
    "if 'T4' in s:\n",
    "  gpu = 'T4'\n",
    "elif 'P100' in s:\n",
    "  gpu = 'P100'\n",
    "elif 'V100' in s:\n",
    "  gpu = 'V100'\n",
    "elif 'A100' in s:\n",
    "  gpu = 'A100'\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "    print('[1;31mit seems that your GPU is not supported at the moment')\n",
    "    time.sleep(5)\n",
    "\n",
    "if (gpu=='T4'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  \n",
    "elif (gpu=='P100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='V100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='A100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Login\n",
    "# -----------------------------------\n",
    "# [boiler plate / not needed]\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use explanatory notes]\n",
    "\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import PIL\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Grid Function \n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion Version \n",
    "# -----------------------------------\n",
    "\n",
    "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2\" \n",
    "\n",
    "# other options are...\n",
    "# \"stabilityai/stable-diffusion-2\"\n",
    "# \"stabilityai/stable-diffusion-2-base\"\n",
    "# \"CompVis/stable-diffusion-v1-4\"\n",
    "# \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup / Check Training Images\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "import requests\n",
    "import glob\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def download_image(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    return None\n",
    "  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "images = list(filter(None,[download_image(url) for url in urls]))\n",
    "save_path = \"./my_concept\"\n",
    "if not os.path.exists(save_path):\n",
    "  os.mkdir(save_path)\n",
    "[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n",
    "grid = image_grid(images, 1, len(images))\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.axis('off')\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates for Training\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dataset\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        learnable_property=\"object\",  # [object, style]\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL.Image.LINEAR,\n",
    "            \"bilinear\": PIL.Image.BILINEAR,\n",
    "            \"bicubic\": PIL.Image.BICUBIC,\n",
    "            \"lanczos\": PIL.Image.LANCZOS,\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            h, w, = (\n",
    "                img.shape[0],\n",
    "                img.shape[1],\n",
    "            )\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLIPTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Tokenizer / Add Placeholder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This section loads the tokenizer and add the placeholder token as a additional special token\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is boiler plate and could use refactoring and explanatory notes\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      8\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m      9\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add the placeholder token in tokenizer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m num_added_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39madd_tokens(placeholder_token)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CLIPTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer / Add Placeholder\n",
    "# -----------------------------------\n",
    "# This section loads the tokenizer and add the placeholder token as a additional special token\n",
    "# This is boiler plate and could use refactoring and explanatory notes\n",
    "\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "# Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Token Ids\n",
    "# -----------------------------------\n",
    "# This code will raise an error if the initializer string is not a single token\n",
    "# It then converts the initializer token and the placeholder token to ids\n",
    "# Clarification is needed on what the initializer token, placeholder token and ids are\n",
    "# This is boiler plate and needs explanatory notes\n",
    "\n",
    "token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
    "\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stable Diffusion Model\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "#@title Load the Stable Diffusion model\n",
    "# Load models and create wrapper for stable diffusion\n",
    "# pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n",
    "# del pipeline\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize Token Embeddings\n",
    "# -----------------------------------\n",
    "# We have added the placeholder_token in the tokenizer\n",
    "# We resize the token embeddings here for a new embedding vector in the token embeddings for our placeholder_token\n",
    "# This is boiler plate and needs further explanatory notes\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Placeholder Token\n",
    "# -----------------------------------\n",
    "# This section initializes the new added placeholder token\n",
    "# This is initialized with the embeddings of the initializer token\n",
    "# This is boiler plate code and needs further explanatory notes\n",
    "\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Model Parameters\n",
    "# -----------------------------------\n",
    "# We are only training the newly added embedding vector\n",
    "# So we freeze the rest of the model parameters\n",
    "# This is boiler plate and needs more notes\n",
    "\n",
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# This freezes vae and unet...\n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(unet.parameters())\n",
    "\n",
    "# This freezes all the parameters except for the token embeddings in text encoder...\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "freeze_params(params_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Training Data\n",
    "# -----------------------------------\n",
    "# This is also boiler plate and needs more notes\n",
    "# The noise scheduler in particular might be a good section to play with\n",
    "\n",
    "# First we create the dataset and the dataloader...\n",
    "train_dataset = TextualInversionDataset(\n",
    "      data_root=save_path,\n",
    "      tokenizer=tokenizer,\n",
    "      size=vae.sample_size,\n",
    "      placeholder_token=placeholder_token,\n",
    "      repeats=100,\n",
    "      learnable_property=what_to_teach, #Option selected above between object and style\n",
    "      center_crop=False,\n",
    "      set=\"train\",\n",
    ")\n",
    "\n",
    "def create_dataloader(train_batch_size=1):\n",
    "    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True) \n",
    "\n",
    "# Now we create the noise scheduler for the training...\n",
    "noise_scheduler = DDPMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actual Training\n",
    "# -----------------------------------\n",
    "# This section is boiler plate but has a lot of potential\n",
    "# There are a lot of parameters that could be tweaked\n",
    "# Good potential user inputs in a gradio interface\n",
    "\n",
    "# First we define our hyperparameters for the training\n",
    "# Tuning the learning_rate and the max_train_steps can improve results\n",
    "\n",
    "# Setting up the training arguments / hyperparameters...\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 5e-04,\n",
    "    \"scale_lr\": True,\n",
    "    \"max_train_steps\": 2000,\n",
    "    \"save_steps\": 250,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"seed\": 42,\n",
    "    \"output_dir\": \"sd-concept-output\"\n",
    "}\n",
    "!mkdir -p sd-concept-output\n",
    "\n",
    "# And then the actual training function...\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def save_progress(text_encoder, placeholder_token_id, accelerator, save_path):\n",
    "    logger.info(\"Saving embeddings\")\n",
    "    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
    "    learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n",
    "    torch.save(learned_embeds_dict, save_path)\n",
    "\n",
    "def training_function(text_encoder, vae, unet):\n",
    "    train_batch_size = hyperparameters[\"train_batch_size\"]\n",
    "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n",
    "    learning_rate = hyperparameters[\"learning_rate\"]\n",
    "    max_train_steps = hyperparameters[\"max_train_steps\"]\n",
    "    output_dir = hyperparameters[\"output_dir\"]\n",
    "    gradient_checkpointing = hyperparameters[\"gradient_checkpointing\"]\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=hyperparameters[\"mixed_precision\"]\n",
    "    )\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    train_dataloader = create_dataloader(train_batch_size)\n",
    "\n",
    "    if hyperparameters[\"scale_lr\"]:\n",
    "        learning_rate = (\n",
    "            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "        text_encoder, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Move vae and unet to device\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # Keep vae in eval mode as we don't train it\n",
    "    vae.eval()\n",
    "    # Keep unet in train mode to enable gradient checkpointing\n",
    "    unet.train()\n",
    "\n",
    "    \n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(text_encoder):\n",
    "                # Convert images to latent space\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states.to(weight_dtype)).sample\n",
    "\n",
    "                 # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                loss = F.mse_loss(noise_pred, target, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                # Zero out the gradients for all token embeddings except the newly added\n",
    "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "                if accelerator.num_processes > 1:\n",
    "                    grads = text_encoder.module.get_input_embeddings().weight.grad\n",
    "                else:\n",
    "                    grads = text_encoder.get_input_embeddings().weight.grad\n",
    "                # Get the index for tokens that we want to zero the grads for\n",
    "                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step % hyperparameters[\"save_steps\"] == 0:\n",
    "                    save_path = os.path.join(output_dir, f\"learned_embeds-step-{global_step}.bin\")\n",
    "                    save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "    # Create the pipeline using using the trained modules and save it.\n",
    "    if accelerator.is_main_process:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            tokenizer=tokenizer,\n",
    "            vae=vae,\n",
    "            unet=unet,\n",
    "        )\n",
    "        pipeline.save_pretrained(output_dir)\n",
    "        # Also save the newly trained embeddings\n",
    "        save_path = os.path.join(output_dir, f\"learned_embeds.bin\")\n",
    "        save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n",
    "\n",
    "import accelerate # <--- adds timestamps\n",
    "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
    "\n",
    "for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n",
    "  if param.grad is not None:\n",
    "    del param.grad # <--- frees some memory\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the New Concept\n",
    "# -----------------------------------\n",
    "# This adds the newly trained concept to hugging face's library of concepts\n",
    "# This makes it possible to reload later for subsequent prompting sessions\n",
    "# It's all boiler plate and could probably be refactored\n",
    "\n",
    "if(save_concept_to_public_library):\n",
    "  from slugify import slugify\n",
    "  from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
    "  from huggingface_hub import create_repo\n",
    "  repo_id = f\"sd-concepts-library/{slugify(name_of_your_concept)}\"\n",
    "  output_dir = hyperparameters[\"output_dir\"]\n",
    "  if(not hf_token_write):\n",
    "    with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
    "  else:\n",
    "    hf_token = hf_token_write\n",
    "  #Join the Concepts Library organization if you aren't part of it already\n",
    "  !curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\n",
    "  images_upload = os.listdir(\"my_concept\")\n",
    "  image_string = \"\"\n",
    "  repo_id = f\"sd-concepts-library/{slugify(name_of_your_concept)}\"\n",
    "  for i, image in enumerate(images_upload):\n",
    "      image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
    "'''\n",
    "  if(what_to_teach == \"style\"):\n",
    "      what_to_teach_article = f\"a `{what_to_teach}`\"\n",
    "  else:\n",
    "      what_to_teach_article = f\"an `{what_to_teach}`\"\n",
    "  readme_text = f'''---\n",
    "license: mit\n",
    "---\n",
    "### {name_of_your_concept} on Stable Diffusion\n",
    "This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Conceptualizer](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb) notebook. You can also train your own concepts and load them into the concept libraries using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).\n",
    "\n",
    "Here is the new concept you will be able to use as {what_to_teach_article}:\n",
    "{image_string}\n",
    "'''\n",
    "  #Save the readme to a file\n",
    "  readme_file = open(\"README.md\", \"w\")\n",
    "  readme_file.write(readme_text)\n",
    "  readme_file.close()\n",
    "  #Save the token identifier to a file\n",
    "  text_file = open(\"token_identifier.txt\", \"w\")\n",
    "  text_file.write(placeholder_token)\n",
    "  text_file.close()\n",
    "  #Save the type of teached thing to a file\n",
    "  type_file = open(\"type_of_concept.txt\",\"w\")\n",
    "  type_file.write(what_to_teach)\n",
    "  type_file.close()\n",
    "  operations = [\n",
    "    CommitOperationAdd(path_in_repo=\"learned_embeds.bin\", path_or_fileobj=f\"{output_dir}/learned_embeds.bin\"),\n",
    "    CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
    "    CommitOperationAdd(path_in_repo=\"type_of_concept.txt\", path_or_fileobj=\"type_of_concept.txt\"),\n",
    "    CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
    "  ]\n",
    "  # create_repo(repo_id,private=True, token=\"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\")\n",
    "  print(\"--------------------->\", hf_token)\n",
    "  create_repo(repo_id,private=True, token=hf_token)\n",
    "  api = HfApi()\n",
    "  api.create_commit(\n",
    "    repo_id=repo_id,\n",
    "    operations=operations,\n",
    "    commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
    "    token=hf_token\n",
    "  )\n",
    "  api.upload_folder(\n",
    "    folder_path=save_path,\n",
    "    path_in_repo=\"concept_images\",\n",
    "    repo_id=repo_id,\n",
    "    token=hf_token\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up the Pipeline\n",
    "# -----------------------------------\n",
    "# Once again this is boiler plate and should have notes added\n",
    "\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    hyperparameters[\"output_dir\"],\n",
    "    scheduler=DPMSolverMultistepScheduler.from_pretrained(hyperparameters[\"output_dir\"], subfolder=\"scheduler\"),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Output Images \n",
    "# -----------------------------------\n",
    "# This function adds output images to Google Drive\n",
    "# This was a custom addition but could still use notes / refactoring\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_image_to_drive(input_img, img_name):\n",
    "  image_to_add = cv2.cvtColor(np.array(input_img), cv2.COLOR_BGR2RGB)\n",
    "  image_name = f'{img_name}.png'\n",
    "  cv2.imwrite(f\"{google_drive_path}/{image_name}\", image_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Stable Diffusion Pipeline\n",
    "# -----------------------------------\n",
    "# This section runs the text-to-image pipeline\n",
    "# It keeps a count and iterates through all the user's text prompts\n",
    "# It produces multiple images for each prompt as specified with random seeds\n",
    "# After the image is produced it gets saved to the specified Google Drive folder\n",
    "# The prompt and image number are saved as the files name\n",
    "# This is modified but needs more refactoring and notes\n",
    "\n",
    "# Set a cumulative count for entire session...\n",
    "count = 0\n",
    "\n",
    "# Check model name before making images so nothing on gets overridden on Google Drive ...\n",
    "if name_of_your_concept != name_of_your_concept_dup:\n",
    "  raise Exception(\"Something is wrong with your concept name, see above.\")\n",
    "\n",
    "for prompt in prompts:\n",
    "  full_prompt = f'{prompt} in the style of <{name_of_your_concept}>'\n",
    "  for i in range(images_per_prompt):\n",
    "    image = pipe(full_prompt, num_inference_steps=30, guidance_scale=7.5).images\n",
    "    file_name = prompt.replace(\" \", \"-\") + f\"-in-the-style-of-{name_of_your_concept}--{i+1}\"\n",
    "    add_image_to_drive(image[0], file_name)\n",
    "    count += 1\n",
    "    print(f\"{file_name} added -- {count}/{total_output_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Images\n",
    "# -----------------------------------\n",
    "# If the desired image count hasn't been reached yet more are made\n",
    "# These are essentially empty prompts in the style of the new concept\n",
    "# Often these produce the most interesting results\n",
    "# Could definitely use cleaning up\n",
    "\n",
    "i = 1\n",
    "while count < total_output_images:\n",
    "    prompt = f\"<{name_of_your_concept}>\"\n",
    "    image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images\n",
    "    file_name = f\"no-prompt-{name_of_your_concept}--{i}\"\n",
    "    add_image_to_drive(image[0], file_name)\n",
    "    count += 1\n",
    "    i += 1\n",
    "    print(f\"{file_name} added -- {count}/{total_output_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplemental Prompts\n",
    "# -----------------------------------\n",
    "# This is turned off by default but can be run repeatedly after training\n",
    "# Section can be turned on by changing the \"False\" below to \"True\"\n",
    "# Add as many additional prompts as you want\n",
    "# They will all be appended with \"in the style of <new-concept>\"\n",
    "# Set the images per prompt to the desired number as well\n",
    "# The cumulative count will continue \n",
    "\n",
    "# This is all custom but could use refactoring\n",
    "\n",
    "\n",
    "run_supplemental_prompts = True\n",
    "\n",
    "images_per_supplemental_prompt = 5\n",
    "\n",
    "# supplemental_prompts = [\n",
    "#     \"empty cities\",\n",
    "#     \"distant forms\",\n",
    "# ]\n",
    "\n",
    "supplemental_prompts = []\n",
    "\n",
    "if run_supplemental_prompts:\n",
    "  _total_supplemental_images = images_per_supplemental_prompt * len(supplemental_prompts)\n",
    "  _supplemental_prompts_completed = 0\n",
    "\n",
    "  for prompt in supplemental_prompts:\n",
    "    full_prompt = f'{prompt} in the style of <{name_of_your_concept}>'\n",
    "    for i in range(images_per_supplemental_prompt):\n",
    "      image = pipe(full_prompt, num_inference_steps=30, guidance_scale=7.5).images\n",
    "      file_name = prompt.replace(\" \", \"-\") + f\"-in-the-style-of-{name_of_your_concept}--{i+1}--{count}\"\n",
    "      add_image_to_drive(image[0], file_name)\n",
    "      _supplemental_prompts_completed += 1\n",
    "      count += 1\n",
    "      print(f\"{file_name} added -- {_supplemental_prompts_completed}/{_total_supplemental_images} -- {count} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Prompts\n",
    "# -----------------------------------\n",
    "# This is similar to the section above but randomizes prompts\n",
    "# It combines \"styles\" and \"subjects\" into randomly paired prompt strings\n",
    "# It will continue \n",
    "# This is turned off by default but can be run repeatedly after training\n",
    "# Section can be turned on by changing the \"False\" below to \"True\"\n",
    "# Add as many additional prompts as you want\n",
    "# They will all be appended with \"in the style of <new-concept>\"\n",
    "# Set the images per prompt to the desired number as well\n",
    "# The cumulative count will continue \n",
    "\n",
    "# This is all custom but could use refactoring\n",
    "\n",
    "\n",
    "run_supplemental_prompts = False\n",
    "\n",
    "import random\n",
    "\n",
    "images_per_supplemental_prompt = 25\n",
    "\n",
    "random_styles = []\n",
    "random_subjects = []\n",
    "\n",
    "if run_supplemental_prompts:\n",
    "  _total_supplemental_images = images_per_supplemental_prompt\n",
    "  _supplemental_prompts_completed = 0\n",
    "\n",
    "\n",
    "  for i in range(images_per_supplemental_prompt):\n",
    "    random_subject = random.choice(random_subjects)\n",
    "    random_style = random.choice(random_styles)\n",
    "\n",
    "    prompt = f\"{random_style} of {random_subject}\"\n",
    "    full_prompt = f'{prompt} in the style of <{name_of_your_concept}> on a bright white background'\n",
    "\n",
    "    image = pipe(full_prompt, num_inference_steps=30, guidance_scale=7.5).images\n",
    "    file_name = prompt.replace(\" \", \"-\") + f\"-in-the-style-of-{name_of_your_concept}--{i+1}--{count}\"\n",
    "    add_image_to_drive(image[0], file_name)\n",
    "    _supplemental_prompts_completed += 1\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    print(f\"{file_name} added -- {_supplemental_prompts_completed}/{_total_supplemental_images}\")\n",
    "\n",
    "  print(\"SUPPLEMENTAL PROMPT IMAGE GENERATION COMPLETE -----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Sets\n",
    "# -----------------------------------\n",
    "# These are some optional ideas for prompts\n",
    "# They can be copied from and added to the prompt sets above\n",
    "# They should be added to freely as they aren't used anywhere in the code\n",
    "\n",
    "_example_prompts = [\n",
    "    \"an abstract painting\",\n",
    "    \"an oil painting\",\n",
    "    \"a dark painting\",\n",
    "    \"a light painting\",\n",
    "    \"another world\",\n",
    "    \"the voice\",\n",
    "    \"voyaging\",\n",
    "    \"winter\",\n",
    "    \"alchemy\",\n",
    "    \"light center\",\n",
    "    \"white fire\",\n",
    "    \"resurgence\",\n",
    "    \"constellations\",\n",
    "    \"mountains\",\n",
    "    \"planets\",\n",
    "    \"rocks\",\n",
    "    \"stars\",\n",
    "    \"abstract geometric forms\",\n",
    "    \"abstract forms\",\n",
    "    \"geometric forms\",\n",
    "    \"sacred forms\",\n",
    "    \"transcendental forms\",\n",
    "    \"sacred visions\",\n",
    "    \"an abstract landscape\",\n",
    "    \"a desert landscape\",\n",
    "    \"the ocean\",\n",
    "    \"sunset clouds\",\n",
    "    \"dawn clouds\",\n",
    "    \"twilight\",\n",
    "    \"dusk\"\n",
    "]\n",
    "\n",
    "_example_styles = [\n",
    "    \"a photograph\",\n",
    "    \"a photographic print\",\n",
    "    \"a black and white photograph\",\n",
    "    \"a sculpture\",\n",
    "    \"a stone sculpture\",\n",
    "    \"a drawing\",\n",
    "    \"a pencil drawing\",\n",
    "    \"a charcoal drawing\",\n",
    "    \"a polaroid photograph\",\n",
    "    \"a concrete sculpture\",\n",
    "    \"an intaglio print\"\n",
    "]\n",
    "\n",
    "_example_subjects = [\n",
    "    \"empty cities\",\n",
    "    \"an empty city\",\n",
    "    \"empty buildings\",\n",
    "    \"an empty building\",\n",
    "    \"an empty landscape\",\n",
    "    \"an intaglio print\",\n",
    "    \"a painting\",\n",
    "    \"a painting\",\n",
    "    \"an oil painting\",\n",
    "    \"an oil painting\",\n",
    "    \"a black and white photograph\",\n",
    "    \"a polaroid photograph\",\n",
    "    \"a pencil drawing\",\n",
    "    \"a charcoal drawing\",\n",
    "    \"distant vessels\",\n",
    "    \"distant detailed forms\",\n",
    "    \"distant intricate forms\",\n",
    "    \"detailed forms\",\n",
    "    \"intricate forms\",\n",
    "    \"an abstract form\",\n",
    "    \"an intricate abstract form\",\n",
    "    \"an ornate abstract form\",\n",
    "    \"a mechanical abstract form\",\n",
    "    \"a geometric abstract form\",\n",
    "    \"a detailed abstract form\",\n",
    "    \"an organic abstract form\",\n",
    "    \"an abstract form with a face\",\n",
    "    \"an intricate abstract form with a face\",\n",
    "    \"an ornate abstract form with a face\",\n",
    "    \"a mechanical abstract form with a face\",\n",
    "    \"a geometric abstract form with a face\",\n",
    "    \"a detailed abstract form with a face\",\n",
    "    \"an organic abstract form with a face\",\n",
    "    \"a horse\",\n",
    "    \"a dog\",\n",
    "    \"a bird\",\n",
    "    \"a cat\",\n",
    "    \"a frog\",\n",
    "    \"a bear\",\n",
    "    \"a dolphin\",\n",
    "    \"a dragon\",\n",
    "    \"a wizard\",\n",
    "    \"a knight\",\n",
    "    \"a bull\",\n",
    "    \"a cow\",\n",
    "    \"a car\",\n",
    "    \"a snail\",\n",
    "    \"a crow\",\n",
    "    \"a rabbit\",\n",
    "    \"a penguin\",\n",
    "    \"a pig\",\n",
    "    \"a chicken\",\n",
    "    \"a rat\",\n",
    "    \"a house\",\n",
    "    \"an abstract shape\",\n",
    "    \"an abstract machine\",\n",
    "    \"a human figure\",\n",
    "    \"an alien\",\n",
    "    \"a robot\",\n",
    "    \"a planet\",\n",
    "    \"a tree\",\n",
    "    \"a mountain\",\n",
    "    \"a rock\",\n",
    "    \"a spaceship\",\n",
    "    \"a hand\",\n",
    "    \"a foot\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bcb852cba4962000fffa2782f733ff943ee72b596ba59c585ffc3382b4703d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
