{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual Inversion Training \n",
    "# -----------------------------------\n",
    "# This notebook was built for stable diffusion fine-tuning\n",
    "# It takes 3-8 training image urls as it's primary input\n",
    "# Additionally it can take any number of text prompts\n",
    "# It outputs any specified number of images\n",
    "# Output images are saved in Google Drive\n",
    "# -----------------------------------\n",
    "# You can run as a Google Colab Notebook\n",
    "# Not 100% cleaned up, definitely a work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources / Reference Notebooks\n",
    "# -----------------------------------\n",
    "# https://huggingface.co/docs/diffusers/training/text_inversion\n",
    "# https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb\n",
    "# https://www.youtube.com/watch?v=_7rMfsA24Ls\n",
    "# https://huggingface.co/\n",
    "# https://www.fast.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Images URL's\n",
    "# -----------------------------------\n",
    "# Paste in urls for the images you want to train your new concept on.\n",
    "# These could be images that inspire you or images of your artwork etc.\n",
    "# Upload your own images to https://imgur.com/ for urls\n",
    "# 5-8 images is usually adequate for a training.\n",
    "# Follow the format below.\n",
    "\n",
    "# urls = [\n",
    "#   \"https://i.imgur.com/xNZW4TW.jpg\",\n",
    "#   \"https://i.imgur.com/9NMokla.jpg\",\n",
    "#   \"https://i.imgur.com/A4p0djw.jpg\",\n",
    "#   \"https://i.imgur.com/ocYQtxp.jpg\",\n",
    "#   \"https://i.imgur.com/CuchxlL.jpg\",\n",
    "#   \"https://i.imgur.com/9uDvP2j.jpg\",\n",
    "#   \"https://i.imgur.com/63W9Cyv.jpg\",\n",
    "# ]\n",
    "\n",
    "urls = [] # <----- paste your image urls here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Prompts\n",
    "# -----------------------------------\n",
    "# Add as many text prompts as desired.\n",
    "# All text prompts will be run after initial training.\n",
    "# Output images will go to your specified Google Drive folder.\n",
    "# All prompts will be appended with \"in the style of <your-model>\"\n",
    "# More prompts can be given later.\n",
    "\n",
    "prompts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Information / Variables\n",
    "# -----------------------------------\n",
    "# [needs refactoring]\n",
    "\n",
    "\n",
    "# Variables for model outputs...\n",
    "images_per_prompt = 5\n",
    "total_output_images = 175\n",
    "ahx_model_number = 11\n",
    "\n",
    "what_to_teach = \"style\" # [\"object\", \"style\"]\n",
    "# `initializer_token` is a word that can summarise what your new concept is, to be used as a starting point\n",
    "initializer_token = \"painting\"\n",
    "\n",
    "\n",
    "\n",
    "# ------ DON'T CHANGE THESE ----------------\n",
    "\n",
    "# Variables to save new model to hugging face...\n",
    "save_concept_to_public_library = True\n",
    "name_of_your_concept = f\"ahx-model-{ahx_model_number}\"\n",
    "name_of_your_concept_dup = f\"{name_of_your_concept}\" # temporary for sanity check before uploading to concept library\n",
    "hf_token_write = \"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\"\n",
    "\n",
    "# Mount log in to google drive to save images...\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') # <-- shouldn't change\n",
    "root_path = '/content/drive/My Drive' # <-- shouldn't change\n",
    "\n",
    "# Make your folder in Google Drive and define here...\n",
    "your_path = f'/stable-diffusion/model-{ahx_model_number}-bulk' # <-- your folder\n",
    "google_drive_path = f'{root_path}{your_path}'\n",
    "\n",
    "# `placeholder_token` is the token you are going to use to represent your new concept (so when you prompt the model, you will say \"A `<my-placeholder-token>` in an amusement park\"). We use angle brackets to differentiate a token from other words/tokens, to avoid collision.\n",
    "placeholder_token = f\"<{name_of_your_concept}>\" # {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Installations\n",
    "# -----------------------------------\n",
    "# [boiler plate / needs refactoring]\n",
    "\n",
    "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
    "!pip install -qq accelerate transformers ftfy\n",
    "!pip install -qq \"ipywidgets>=7,<8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Optimization\n",
    "# -----------------------------------\n",
    "# [boiler plate / needs refactoring]\n",
    "\n",
    "\n",
    "!pip install -U --pre triton\n",
    "\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "s = getoutput('nvidia-smi')\n",
    "if 'T4' in s:\n",
    "  gpu = 'T4'\n",
    "elif 'P100' in s:\n",
    "  gpu = 'P100'\n",
    "elif 'V100' in s:\n",
    "  gpu = 'V100'\n",
    "elif 'A100' in s:\n",
    "  gpu = 'A100'\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "    print('[1;31mit seems that your GPU is not supported at the moment')\n",
    "    time.sleep(5)\n",
    "\n",
    "if (gpu=='T4'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  \n",
    "elif (gpu=='P100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='V100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='A100'):\n",
    "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Login\n",
    "# -----------------------------------\n",
    "# [boiler plate / not needed]\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use explanatory notes]\n",
    "\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import PIL\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Grid Function \n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion Version \n",
    "# -----------------------------------\n",
    "\n",
    "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2\" \n",
    "\n",
    "# other options are...\n",
    "# \"stabilityai/stable-diffusion-2\"\n",
    "# \"stabilityai/stable-diffusion-2-base\"\n",
    "# \"CompVis/stable-diffusion-v1-4\"\n",
    "# \"runwayml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup / Check Training Images\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "import requests\n",
    "import glob\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def download_image(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    return None\n",
    "  return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "images = list(filter(None,[download_image(url) for url in urls]))\n",
    "save_path = \"./my_concept\"\n",
    "if not os.path.exists(save_path):\n",
    "  os.mkdir(save_path)\n",
    "[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n",
    "grid = image_grid(images, 1, len(images))\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.axis('off')\n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates for Training\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]\n",
    "\n",
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dataset\n",
    "# -----------------------------------\n",
    "# [boiler plate / could use refactor]\n",
    "\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        learnable_property=\"object\",  # [object, style]\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL.Image.LINEAR,\n",
    "            \"bilinear\": PIL.Image.BILINEAR,\n",
    "            \"bicubic\": PIL.Image.BICUBIC,\n",
    "            \"lanczos\": PIL.Image.LANCZOS,\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            h, w, = (\n",
    "                img.shape[0],\n",
    "                img.shape[1],\n",
    "            )\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLIPTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Tokenizer / Add Placeholder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This section loads the tokenizer and add the placeholder token as a additional special token\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is boiler plate and could use refactoring and explanatory notes\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      8\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m      9\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add the placeholder token in tokenizer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m num_added_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39madd_tokens(placeholder_token)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CLIPTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer / Add Placeholder\n",
    "# -----------------------------------\n",
    "# This section loads the tokenizer and add the placeholder token as a additional special token\n",
    "# This is boiler plate and could use refactoring and explanatory notes\n",
    "\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "# Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Token Ids\n",
    "# -----------------------------------\n",
    "# This code will raise an error if the initializer string is not a single token\n",
    "# It then converts the initializer token and the placeholder token to ids\n",
    "# Clarification is needed on what the initializer token, placeholder token and ids are\n",
    "# This is boiler plate and needs explanatory notes\n",
    "\n",
    "token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
    "\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stable Diffusion Model\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "#@title Load the Stable Diffusion model\n",
    "# Load models and create wrapper for stable diffusion\n",
    "# pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n",
    "# del pipeline\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize Token Embeddings\n",
    "# -----------------------------------\n",
    "# We have added the placeholder_token in the tokenizer\n",
    "# We resize the token embeddings here for a new embedding vector in the token embeddings for our placeholder_token\n",
    "# This is boiler plate and needs further explanatory notes\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Placeholder Token\n",
    "# -----------------------------------\n",
    "# This section initializes the new added placeholder token\n",
    "# This is initialized with the embeddings of the initializer token\n",
    "# This is boiler plate code and needs further explanatory notes\n",
    "\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Model Parameters\n",
    "# -----------------------------------\n",
    "# We are only training the newly added embedding vector\n",
    "# So we freeze the rest of the model parameters\n",
    "# This is boiler plate and needs more notes\n",
    "\n",
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# This freezes vae and unet...\n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(unet.parameters())\n",
    "\n",
    "# This freezes all the parameters except for the token embeddings in text encoder...\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "freeze_params(params_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Training Data\n",
    "# -----------------------------------\n",
    "# This is also boiler plate and needs more notes\n",
    "# The noise scheduler in particular might be a good section to play with\n",
    "\n",
    "# First we create the dataset and the dataloader...\n",
    "train_dataset = TextualInversionDataset(\n",
    "      data_root=save_path,\n",
    "      tokenizer=tokenizer,\n",
    "      size=vae.sample_size,\n",
    "      placeholder_token=placeholder_token,\n",
    "      repeats=100,\n",
    "      learnable_property=what_to_teach, #Option selected above between object and style\n",
    "      center_crop=False,\n",
    "      set=\"train\",\n",
    ")\n",
    "\n",
    "def create_dataloader(train_batch_size=1):\n",
    "    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True) \n",
    "\n",
    "# Now we create the noise scheduler for the training...\n",
    "noise_scheduler = DDPMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bcb852cba4962000fffa2782f733ff943ee72b596ba59c585ffc3382b4703d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
