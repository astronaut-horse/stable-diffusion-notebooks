{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWxYpN0DFtn3"
      },
      "outputs": [],
      "source": [
        "#@title 1. General Setup\n",
        "\n",
        "!pip install -qq diffusers==0.11.1 transformers ftfy accelerate\n",
        "!pip install -Uq diffusers transformers\n",
        "!pip install -Uq gradio\n",
        "!pip install -Uq accelerate\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipeline = StableDiffusionPipeline\n",
        "\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import init_empty_weights\n",
        "import gradio\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# FOR DEPLOYMENT: uncomment these and delete the notebook_login() below\n",
        "# api_key = os.environ['api_key']\n",
        "# my_token = api_key\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2\"\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Tell it What Concepts to Load\n",
        "\n",
        "models_to_load = [\n",
        "    \"ahx-model-3\",\n",
        "    \"ahx-model-5\",\n",
        "    \"ahx-model-6\",\n",
        "    \"ahx-model-7\",\n",
        "    \"ahx-model-8\",\n",
        "    \"ahx-model-9\",\n",
        "    \"ahx-model-10\",\n",
        "    \"ahx-model-11\",\n",
        "]\n",
        "\n",
        "models_to_load = [f\"sd-concepts-library/{model}\" for model in models_to_load]\n",
        "completed_concept_pipes = {}"
      ],
      "metadata": {
        "id": "MS_dN43lSocs"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Load the Concepts as Distinct Pipes\n",
        "\n",
        "for repo_id_embeds in models_to_load:\n",
        "  print(f\"loading {repo_id_embeds}\")\n",
        "  print(\"----------------------\")\n",
        "  # repo_id_embeds = \"sd-concepts-library/ahx-model-3\"\n",
        "\n",
        "  embeds_url = \"\" #Add the URL or path to a learned_embeds.bin file in case you have one\n",
        "  placeholder_token_string = \"\" #Add what is the token string in case you are uploading your own embed\n",
        "\n",
        "  downloaded_embedding_folder = \"./downloaded_embedding\"\n",
        "  if not os.path.exists(downloaded_embedding_folder):\n",
        "    os.mkdir(downloaded_embedding_folder)\n",
        "  if(not embeds_url):\n",
        "    embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"learned_embeds.bin\")\n",
        "    token_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"token_identifier.txt\")\n",
        "    !cp $embeds_path $downloaded_embedding_folder\n",
        "    !cp $token_path $downloaded_embedding_folder\n",
        "    with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:\n",
        "      placeholder_token_string = file.read()\n",
        "  else:\n",
        "    !wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url\n",
        "\n",
        "  learned_embeds_path = f\"{downloaded_embedding_folder}/learned_embeds.bin\"\n",
        "\n",
        "  # ----\n",
        "\n",
        "  tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"tokenizer\",\n",
        "  )\n",
        "  text_encoder = CLIPTextModel.from_pretrained(\n",
        "      pretrained_model_name_or_path, subfolder=\"text_encoder\", torch_dtype=torch.float16\n",
        "  )\n",
        "\n",
        "  # ----\n",
        "\n",
        "  def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "    loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
        "    \n",
        "    # separate token and the embeds\n",
        "    trained_token = list(loaded_learned_embeds.keys())[0]\n",
        "    embeds = loaded_learned_embeds[trained_token]\n",
        "\n",
        "    # cast to dtype of text_encoder\n",
        "    dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "    embeds.to(dtype)\n",
        "\n",
        "    # add the token in tokenizer\n",
        "    token = token if token is not None else trained_token\n",
        "    num_added_tokens = tokenizer.add_tokens(token)\n",
        "    if num_added_tokens == 0:\n",
        "      raise ValueError(f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n",
        "    \n",
        "    # resize the token embeddings\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    \n",
        "    # get the id for the token and assign the embeds\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "\n",
        "  load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)\n",
        "\n",
        "  # FOR DEPLOYMENT: add use_auth_token=my_token to pipe keyword args\n",
        "    # ie --> pipe = pipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=my_token).to(\"cuda\")\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  completed_concept_pipes[repo_id_embeds] = pipe\n",
        "  print(\"--> complete !\")\n",
        "  print(\"----------------------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qNKZdoFaHW57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Print Available Concept Strings\n",
        "\n",
        "print(\"AVAILABLE CONCEPTS TO SELECT FROM\")\n",
        "print(\"copy one and paste below under 'model'\")\n",
        "print(\"------------------------------------------------------\")\n",
        "# list(completed_concept_pipes)\n",
        "for model in completed_concept_pipes:\n",
        "  print(f\"{model}\")"
      ],
      "metadata": {
        "id": "e5WsF7DQV9BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Optionally Test without Gradio\n",
        "\n",
        "model = \"\" #@param {type: \"string\"}\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if prompt and model:\n",
        "  if model not in completed_concept_pipes:\n",
        "    raise ValueError(\"Invalid Model Name\")\n",
        "\n",
        "  model_token = model.split(\"/\")[1]\n",
        "  prompt = f\"{prompt} in the style of <{model_token}>\"\n",
        "\n",
        "  if model == \"sd-concepts-library/ahx-model-5\":\n",
        "    prompt = f\"{prompt} in the style of <ahx-model-4>\"\n",
        "\n",
        "  num_samples = 1\n",
        "  num_rows = 1\n",
        "\n",
        "  all_images = [] \n",
        "  pipe = completed_concept_pipes[model]\n",
        "\n",
        "  for _ in range(num_rows):\n",
        "      images = pipe(prompt, num_images_per_prompt=num_samples, height=512, width=512, num_inference_steps=30, guidance_scale=7.5).images\n",
        "      all_images.extend(images)\n",
        "\n",
        "  grid = image_grid(all_images, num_samples, num_rows)\n",
        "  grid"
      ],
      "metadata": {
        "id": "QxfbaTopIhsE"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6. Define Custom CSS for Gradio\n",
        "\n",
        "use_custom_css = True\n",
        "\n",
        "gradio_css = \"\"\"\n",
        "  #output-image {\n",
        "    border: 1px solid black;\n",
        "    background-color: white;\n",
        "    width: 500px;\n",
        "    display: block;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "  }\n",
        "\"\"\"\n",
        "\n",
        "gradio_css_alternative = \"\"\"\n",
        "  #go-button {\n",
        "    background-color: white;\n",
        "    border-radius: 0;\n",
        "    border: none;\n",
        "    font-family: serif;\n",
        "    background-image: none;\n",
        "    font-weight: 100;\n",
        "    width: fit-content;\n",
        "    display: block;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "    text-decoration: underline;\n",
        "    box-shadow: none;\n",
        "    color: blue;\n",
        "  }\n",
        "  .rounded-lg {\n",
        "    border: none;\n",
        "  }\n",
        "  .gr-box {\n",
        "    border-radius: 0;\n",
        "    border: 1px solid black;\n",
        "  }\n",
        "  .text-gray-500 {\n",
        "    color: black;\n",
        "    font-family: serif;\n",
        "    font-size: 15px;\n",
        "  }\n",
        "  .border-gray-200 {\n",
        "    border: 1px solid black;\n",
        "  }\n",
        "  .bg-gray-200 {\n",
        "    background-color: white;\n",
        "    --tw-bg-opacity: 0;\n",
        "  }\n",
        "  footer {\n",
        "    display: none;\n",
        "  }\n",
        "  footer {\n",
        "    opacity: 0;\n",
        "  }\n",
        "  #output-image {\n",
        "    border: 1px solid black;\n",
        "    background-color: white;\n",
        "    width: 500px;\n",
        "    display: block;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "  }\n",
        "  .absolute {\n",
        "    display: none;\n",
        "  }\n",
        "  #input-text {\n",
        "    width: 500px;\n",
        "    display: block;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "    padding: 0 0 0 0;\n",
        "  }\n",
        "  .py-6 {\n",
        "    padding-top: 0;\n",
        "    padding-bottom: 0;\n",
        "  }\n",
        "  .px-4 {\n",
        "    padding-left: 0;\n",
        "    padding-right: 0;\n",
        "  }\n",
        "  .rounded-lg {\n",
        "    border-radius: 0;\n",
        "  }\n",
        "  .gr-padded {\n",
        "    padding: 0 0;\n",
        "    margin-bottom: 12.5px;\n",
        "  }\n",
        "  .col > *, .col > .gr-form > * {\n",
        "    width: 500px;\n",
        "    margin-left: auto;\n",
        "    margin-right: auto;\n",
        "  }\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "vMOiaM6-siUu"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7. Build and Launch the Gradio Interface\n",
        "\n",
        "DROPDOWNS = {}\n",
        "\n",
        "for model in models_to_load:\n",
        "  token = model.split(\"/\")[1]\n",
        "  DROPDOWNS[model] = f\" in the style of <{token}>\"\n",
        "\n",
        "if \"sd-concepts-library/ahx-model-5\" in DROPDOWNS:\n",
        "  DROPDOWNS[\"sd-concepts-library/ahx-model-5\"] = f\"{prompt} in the style of <ahx-model-4>\"\n",
        "\n",
        "def image_prompt(prompt, dropdown):\n",
        "  prompt = prompt + DROPDOWNS[dropdown]\n",
        "  pipe = completed_concept_pipes[dropdown]\n",
        "  return pipe(prompt=prompt, height=512, width=512).images[0]\n",
        "\n",
        "with gradio.Blocks(css=gradio_css if use_custom_css else \"\") as demo:\n",
        "  dropdown = gradio.Dropdown(list(DROPDOWNS), label=\"choose style...\")\n",
        "  prompt = gradio.Textbox(label=\"image prompt...\", elem_id=\"input-text\")\n",
        "  output = gradio.Image(elem_id=\"output-image\")\n",
        "  go_button = gradio.Button(\"draw it!\", elem_id=\"go-button\")\n",
        "  go_button.click(fn=image_prompt, inputs=[prompt, dropdown], outputs=output)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "XmDPbjL5ieor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v2R5GKlmHJYu"
      }
    }
  ]
}
